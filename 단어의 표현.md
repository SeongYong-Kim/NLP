# 단어의 표현

![image-20201002200633780](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\image-20201002200633780.png)

이미지 출처 : https://wikidocs.net/45609



* Local Representation : 해당 단어 그 자체만 보고 특정값을 매핑하는 방법
* Continuous Representation(Distributed Representation) : 해당 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법. Continuous Representation이 Distributed Representation보다 더 포괄적인 개념이라는 주장도 있음.



### Bag of Words

단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법. 순서는 중요하지 않음

(1)  단어에 고유한 정수 인덱스 부여

(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터 생성

```shell
from konlpy.tag import Okt
import re  
okt=Okt()  

token=re.sub("(\.)","","정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.")  
# 정규 표현식을 통해 온점을 제거하는 정제 작업
token=okt.morphs(token)  
# OKT 형태소 분석기를 통해 토큰화 작업

word2index={}  
bow=[]  
for voca in token:  
         if voca not in word2index.keys():  
             word2index[voca]=len(word2index)  
# token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘깁니다.   
             bow.insert(len(word2index)-1,1)
# BoW 전체에 전부 기본값 1을 설정 
         else:
            index=word2index.get(voca)
# 재등장하는 단어의 인덱스를 받아옵니다.
            bow[index]=bow[index]+1
# 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  
print(word2index) 

<result>
('정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9)  
```

```shell
bow

<result>
[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]
```

BoW는 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰임. 즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰임

#### 1. CountVectorizer 클래스로 BoW 만들기

```shell
from sklearn.feature_extraction.text import CountVectorizer

corpus = ['you know I want your love. because I love you.']

vector = CountVectorizer()

print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록
print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.

<result>
[[1 1 2 1 2 1]]
{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}
```



#### 2. Stopword를 제거한 BoW 만들기

* 사이킷런

```shell
#불용어 직접 설정
from sklearn.feature_extraction.text import CountVectorizer

text=["Family is not an important thing. It's everything."]

vect = CountVectorizer(stop_words=["the", "a", "an", "is", "not"])

print(vect.fit_transform(text).toarray()) 
print(vect.vocabulary_)
```

```shell
#CountVectorizer에서 제공하는 자체 불용어 사용
from sklearn.feature_extraction.text import CountVectorizer

text=["Family is not an important thing. It's everything."]

vect = CountVectorizer(stop_words="english")

print(vect.fit_transform(text).toarray())
print(vect.vocabulary_)
```

```shell
#NLTK에서 지원하는 불용어 사용
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords

text=["Family is not an important thing. It's everything."]

sw = stopwords.words("english")

vect = CountVectorizer(stop_words =sw)

print(vect.fit_transform(text).toarray()) 
print(vect.vocabulary_)
```



### 문서 단어 행렬(DTM)

one-hot vector와 함께 sparse representation임. 0이 대부분을 차지한다는 의미

![image-20201002204012289](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\image-20201002204012289.png)

이미지 출처 : https://wikidocs.net/45609



문제점 :  공간낭비

​				불용어로 인해 문서간 유사도 계산 힘듦



### TF-IDF

DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법. 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 사용

문서를 d, 단어를 t, 문서의 총 개수를 n이라고 표현할 때

* tf(d,f) : 특정 문서 d에서의 특정 단어 t의 등장 횟수

* df(t) : 특정 단어 t가 등장한 문서의 수

* idf(d,t) : df(t)에 반비례하는 수

  ![image-20201002205758491](C:\Users\pc\AppData\Roaming\Typora\typora-user-images\image-20201002205758491.png)

  이때, n = l + df일 경우 가중치가 0이됨. 이런 경우를 방지하기 위해 idf에 +1 하여 구현하는 방법도 있음. 실제로 사이킷런에서 사용됨.



pandas 이용해 TF-IDF구현

```shell
import pandas as pd
from math import log

docs = [
  '먹고 싶은 사과',
  '먹고 싶은 바나나',
  '길고 노란 바나나 바나나',
  '저는 과일이 좋아요'
] 
vocab = list(set(w for doc in docs for w in doc.split())) #단어쪼개기
vocab.sort() #가나다순 정렬
```

```shell
#tf idf 함수생성
N = len(docs) # 총 문서의 수

def tf(t, d):
    return d.count(t)

def idf(t):
    df = 0
    for doc in docs:
        df += t in doc
    return log(N/(df + 1))

def tfidf(t, d):
    return tf(t,d)* idf(t)
```

```shell
#tf
result = []
for i in range(N): # 각 문서에 대해서 아래 명령을 수행
    result.append([]) # 문서 자리 생성
    d = docs[i]
    for j in range(len(vocab)): # 한 문서에 모든 단어의 경우 생각
        t = vocab[j]        
        result[-1].append(tf(t, d)) # 각 단어에 대한 tf 구하기

tf_ = pd.DataFrame(result, columns = vocab)
tf_
```

```shell
#idf
result = []
for j in range(len(vocab)):
    t = vocab[j]
    result.append(idf(t))

idf_ = pd.DataFrame(result, index = vocab, columns = ["IDF"])
idf_
```

```shell
#tf-idf
result = []
for i in range(N):
    result.append([])
    d = docs[i]
    for j in range(len(vocab)):
        t = vocab[j]

        result[-1].append(tfidf(t,d))

tfidf_ = pd.DataFrame(result, columns = vocab)
tfidf_
```



사이킷런을 이용해 TF-IDF구현

```shell
#DTM
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'you know I want your love',
    'I like you',
    'what should I do ',    
]

vector = CountVectorizer()
print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록
print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여줌
```

```shell
#TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    'you know I want your love',
    'I like you',
    'what should I do ',    
]

tfidfv = TfidfVectorizer().fit(corpus)
print(tfidfv.transform(corpus).toarray())
print(tfidfv.vocabulary_)
```

