# 딥러닝의 학습 방법

### 1. Foward Propagation



### 2. Loss Function

* MSE : ![img](https://wikidocs.net/images/page/24987/mse.PNG)

* Cross-Entropy :![img](https://wikidocs.net/images/page/24987/%ED%81%AC%EB%A1%9C%EC%8A%A4%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC.PNG)

  

### 3. Optimizer

학습은 어떤 옵티마이저를 사용하느냐에 따라 달라짐, Batch란 파라미터 값을 조정하기 위해 사용하는 데이터의 양

* Batch Gradient Descent : 옵티마이저 중 하나로 전체 데이터를 고려. 한 번의 에포크(훈련 횟수)에 모든 파라미터를 업데이트 한 번 수행. 오래걸리지만 글로벌 미니멈을 찾을 수 있음

  ```shell
  model.fit(X_train, y_train, batch_size = len(trainX))
  ```

* Stochastic Grandient Descent(SGD) : 랜덤으로 하나의 데이터에 의해 파라미터 조정

  ```shell
  model.fit(X_train, y_train, batch_size=1)
  ```

* Mini-Batch Grandient Descent : 정해진 양에 대해서만 계산하여 파라미터 조정

  ```shell
  model.fit(X_train, y_train, batch_size=32) #32를 배치 크기로 하였을 경우
  ```

* Momentum : 관성이용. SGD에서 계산된 접선의 기울기에 전의 접선의 기울기값을 일정한 비율만큼 반영. 로컬 미니멈에서 탈출하는 효과를 얻음

  ```shell
  keras.optimizers.SGD(lr = 0.01, momentum= 0.9)
  #lr : 학습률
  #momentum : 가속화 비율
  #decay : 학습률의 감소율
  #nesterov : 네스테로프 모멘텀의 적용 여부, 모멘텀 작용지점에서 gradient 계산
  ```

* Adagrad : 파라미터들이 의미하는 바가 다름. 모든 파라미터에 동일한 learning rate을 적용하는 것은 비효율적이므로 다른 학습률을 적용시킴. 변화가 많은 파라미터는 학습률이 작게 설정되고 변화가 적은 파라미터는 학습률을 높게 설정.

  ```shell
  keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)
  # lr: 0보다 크거나 같은 float 값. 학습률.
  # epsilon: 0으로 나누는 것 방지. None인 경우 K.epsilon()이 사용됩니다.
  # decay: 0보다 크거나 같은 float 값. 업데이트마다 적용되는 학습률의 감소율입니다.
  ```

* RMSprop : Adagrad에서 시간이 좀 흐른 후 학습률이 지나치게 떨어지는 단점을 개선한 옵티마이저

  ```shell
  keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)
  ```

* Adam : RMSprop과 Momentum을 합친 방법. 방향과 학습률 두 가지를 모두 잡기 위한 방법

  ```shell
  keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
  ```

  

### 4. BackPropagation



### 5. Epochs and Batch size and Iteration

epoch : 순전파와 역전파가 끝난 상태, 학습 횟수

Batch size : 전체 데이터가 2000일때 배치 크기를 200으로 하면 배치의 수는 10

Iteration : 배치의 수